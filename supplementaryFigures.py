#Generates figures S1, S3, and S4

#import python packages
import pickle
import matplotlib as mpl 
import seaborn as sns
import operator as op
import pandas as pd
import glob
import flatdict
import os
import scipy.stats
import networkx as nx
import operator
from sets import Set
import scipy.stats as stat
import requests
import argparse as argparse
from scipy.stats import variation
import numpy as np
import csv as csv
import matplotlib.pyplot as plt
import random
from time import *
import glob
import seaborn as sns
import re
import requests
import urllib2

# import other pieces of our software
from utils import *
import networkConstructor as nc
from analysis_accuracy import *


def flatten(foo):
    #Utility function - returns a generator, remember to cast the return value as a list when using this function
    for x in foo:
        if hasattr(x, '__iter__'):
            for y in flatten(x):
                yield y
        else:
            yield x

def findEnds2(model, node, indiv):
    """
    #find the end of a node in the bitstring
    """
    node=model.nodeList.index(node)
    if node==len(model.nodeList)-1:
        end1=len(indiv) 
    else:
        end1=model.individualParse[node+1]
    start1=model.individualParse[node]
    return start1, end1

def getCodes():
    #"""looks for pathways that have gpickles generated originally"""
    codes=[]
    for file in os.listdir("gpickles"):
        if file.endswith(".gpickle"):
            if os.path.isfile('pickles/'+file[:-8]+'_1_local1.pickle'):
                codes.append(file[:-8])
            else:
                print(file[:-8]+' has no output')
    print(codes)
    return(codes)

def ruleScore6(graph):

    #Function to calculate total ancestor overlap as described in the main text

    # recursive search
    scoreFunction={}
    for n1 in list(graph.nodes()):
        preds_n1_dict=graph.predecessors(n1)
        preds_n1=list(flatten(preds_n1_dict))
        if (len(preds_n1) >= 1):
            scoreFunction[n1]=0
            for pred1 in preds_n1:
                for pred2 in preds_n1:
                    if pred1 != pred2:
                        temp1=set(nx.dfs_predecessors(graph,pred1).keys() + nx.dfs_predecessors(graph,pred1).values())
                        temp2=set(nx.dfs_predecessors(graph,pred2).keys() + nx.dfs_predecessors(graph,pred2).values())
                        scoreFunction[n1]=scoreFunction[n1]+len(list(temp1.intersection(temp2)))
                        if pred1 in nx.dfs_predecessors(graph,pred2):
                            scoreFunction[n1]=scoreFunction[n1]+1
                        if pred2 in nx.dfs_predecessors(graph,pred1):
                            scoreFunction[n1]=scoreFunction[n1]+1
        else:
            scoreFunction[n1]=0
    return(scoreFunction)

def scoreERS(codes, iterations=25):

    #Utility to read importance scores generated by BONITA, and calculate total ancestor overlap in preparation for plotting

    allRes={}
    for code in codes:
        print(code)
        temp_df2=pd.DataFrame()
        allRes[str(code)]={}
        originalGraph=nx.read_gpickle("gpickles/"+code+".gpickle")
        scoreFunction6=ruleScore6(originalGraph)
        graph=originalGraph
        graph = max(nx.weakly_connected_component_subgraphs(originalGraph), key=len) # get around the problem of disconnected graphs

        if len(graph) >= 3:

            for iteration in range(1, iterations+1):

                allRes[str(code)][str(iteration)]={}
                pickleFile=str('pickles/'+code+'_'+str(iteration)+'_local1.pickle')
                outputList=pickle.load(open(pickleFile, 'rb')) #python2 version #outputList=pickle.load(open(pickleFile, 'rb'), encoding='latin1') = python3 version
                bruteOut1,dev,storeModel, storeModel3, equivalents, dev2 = [outputList[k] for k in range(len(outputList))]
                model1=modelHolder(storeModel3)

                if os.path.isfile("pickles/'+code+'_'+str(iteration)+'_scores1.pickle"):
                    pathVals=pickle.Unpickler(open('pickles/'+code+'_'+str(iteration)+'_scores1.pickle', "rb")).load()
                    ImportanceVals={}
                else:
                    print("Importance scores not found, setting all values to 0")
                    isGeneric4=True
                    pathVals=[0] * len(model1.nodeList)

                for node in range(0,len(model1.nodeList)):
                    allRes[str(code)][str(iteration)][str(model1.nodeList[node])]=[]
                    start1,end1=findEnds2(model1, model1.nodeList[node], equivalents[node])
                    ers=equivalents[node] # find the bitstring for just this node
                    inEdges=findInEdges(model1, model1.nodeList.index(model1.nodeList[node]))    
                    plainRules=[]
                    for rule in ers:
                        plainRules.append(writeNode(model1.nodeList.index(model1.nodeList[node]), rule, model1))
                    ruleLengths=len(ers)
                    ersAllNodes=plainRules
                    rnAllNodes=[pr.count("or")+ 1 for pr in plainRules]
                    ImportanceVals=pathVals[node]
                    inDegree=originalGraph.in_degree(model1.nodeList[node])
                    if model1.nodeList[node] in graph.nodes(): #remember that we have just selected the largest component of the graph for graph theoretic analysis
                        allRes[str(code)][str(iteration)][str(model1.nodeList[node])]=[ruleLengths, rnAllNodes, ImportanceVals, inDegree, scoreFunction6[model1.nodeList[node]]]
                    else:
                        allRes[str(code)][str(iteration)][str(model1.nodeList[node])]=[ruleLengths, rnAllNodes, ImportanceVals, float('NaN'), scoreFunction6[model1.nodeList[node]]]
            
        else:
            continue

    return(allRes)

def makeRNdataframe(scoreERS_output, ifPrint=1, filename="scoreERS.csv"):
    # Utility to processs data from pickles generated by BONITA
    """Takes the three-layered dictionary generated by scoreERS as input"""
    allRes_flat=flatdict.FlatDict(scoreERS_output)
    allRes_df=pd.DataFrame(allRes_flat.iteritems())

    allRes_df[["ruleLengths", "rnAllNodes", "ImportanceVals","inDegree", "scoreFunction6"]]=pd.DataFrame([item for sublist in allRes_df[[1]].values for item in sublist], index=allRes_df.index)
    allRes_df[["Pathway", "Iteration", "Node"]]=pd.DataFrame([x[0].split(":", 2) for x in allRes_df[[0]].values], index=allRes_df.index)

    allRes_df[["ruleLengths", "ImportanceVals", "inDegree", "scoreFunction6"]]=allRes_df[["ruleLengths", "ImportanceVals", "inDegree", "scoreFunction6"]].apply(pd.to_numeric, axis=1)

    if ifPrint:
        # or if filename:
        resFile=open(str(filename),"w+")
        allRes_df.to_csv(resFile)
        resFile.close()
    
    return(allRes_df)

def Fig2PanelD(allRes_df):
    #Main test fig2, panel D: log2(score+1) vs log2(ruleLengths)

    #Set up plotting values
    temp1=np.array(list(allRes_df.loc[:,'scoreFunction6'].values))
    temp1=np.add(temp1, 1)
    temp1=np.log2(temp1)
    temp2=np.log2(np.array(list(allRes_df.loc[:,'ruleLengths'].values)))
    
    sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})
    sns.set_palette(sns.color_palette("Greys_r", 6))
    sns.set_style("ticks")
    sns.set_context("paper")

    fig, ax = plt.subplots(figsize=[2.6,2])
    sns.set_style("ticks")
    sns.set(font_scale=0.8) 
    plt.rc('font', size=8)          # controls default text sizes
    plt.rc('axes', titlesize=8)     # fontsize of the axes title
    plt.rc('axes', labelsize=8)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=8)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=8)    # fontsize of the tick labels
    plt.rc('legend', fontsize=8)    # legend fontsize
    plt.rc('figure', titlesize=8)  # fontsize of the figure title
    ax=sns.regplot(x=temp1, y=temp2, fit_reg=False, color='black', ci=None, scatter_kws={"alpha": "1", 's':1}, marker='o')
    sns.set_style("ticks")
    plt.rc('font', size=8)          # controls default text sizes
    plt.rc('axes', titlesize=8)     # fontsize of the axes title
    plt.rc('axes', labelsize=8)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=8)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=8)    # fontsize of the tick labels
    plt.rc('legend', fontsize=8)    # legend fontsize
    plt.rc('figure', titlesize=8)  # fontsize of the figure title
    plt.tight_layout()
    plt.xlim(left=0)
    plt.ylim(bottom=0)
    finishPlot("Log2(Total Ancestor Overlap)", "Log2(Length of ERS)", "Fig2_PanelD.png") #finishPlot(xlabeler, ylabeler, plotname)
    plt.close()

def supplementary1(allRes_df):
    #Overall distribution of rule number, corresponding to supplementary figure 1
    #This plot shows the median rule number for nodes in a pathway - clearly showing the shift to the left
        
    allRes_df=allRes_df.groupby(['Node', 'Pathway']) #Aggregate results by iteration


    dataframes = [group for _, group in allRes_df]

    rn_by_in_degree=[]
    in_degree=[]
    temp_rnAllNodes=[]

    for data in dataframes:
        path=set(list(data["Pathway"].values))
        for n in set(list(data["Node"].values)):
            is_node=data['Node']==n
            is_lowInDegree=data['inDegree']>=3
            data=data[is_node]
            data=data[is_lowInDegree]
            if (data.shape[0]>0):
                indegree=int(set(data['inDegree'].values).pop())
                temp=np.concatenate(list(data.loc[:, "rnAllNodes"].values))
                in_degree.extend([indegree]*len(temp))
                rn_by_in_degree.extend(list(temp))
                temp=np.array(temp)
                temp=np.median(temp)
                temp_rnAllNodes.append(float(temp))
                #in_degree.extend(set(data.loc[:, "inDegree"].values).pop())
            else:
                continue
    
    print("Mean of rule number:")
    print(np.mean(np.array(temp_rnAllNodes)))
    sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})
    sns.set_palette(sns.color_palette("Greys_r", 6))
    sns.set_style("ticks")
    sns.set_context(context='paper', font_scale=0.8)
    fig, ax = plt.subplots(figsize=[2.6,2])
    sns.set_style("ticks")
    sns.set(font_scale=0.8) 
    plt.rc('font', size=8)          # controls default text sizes
    plt.rc('axes', titlesize=8)     # fontsize of the axes title
    plt.rc('axes', labelsize=8)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=8)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=8)    # fontsize of the tick labels
    plt.rc('legend', fontsize=8)    # legend fontsize
    plt.rc('figure', titlesize=8)  # fontsize of the figure title
    b_width = 1  # chose an arbitrary value here
    my_bins = np.arange(min(rn_by_in_degree), 7 + b_width, b_width)
    fig2=sns.distplot(rn_by_in_degree, color='grey', kde=False, hist_kws={"alpha": 1, "color": "grey"}, bins=my_bins)
    plt.legend(bbox_to_anchor=(1.1, .8), bbox_transform=plt.gcf().transFigure)
    plt.xlim(1,7)
    sns.set_style("ticks")
    plt.rc('font', size=8)          # controls default text sizes
    plt.rc('axes', titlesize=8)     # fontsize of the axes title
    plt.rc('axes', labelsize=8)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=8)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=8)    # fontsize of the tick labels
    plt.rc('legend', fontsize=8)    # legend fontsize
    plt.rc('figure', titlesize=8)  # fontsize of the figure title
    finishPlot("Rule Number", "Frequency", "supplementary1.png") #finishPlot(xlabeler, ylabeler, plotname)

def supplementary4():

    #Correlation of BONITA's node impact score with graph theoretical measures

    codes=getCodes()
    maxReps=6
    ersAllNodes={}
    rnAllNodes={}
    ruleLengths={}
    allRes={} #three-layered dictionary to store results

    for code in codes:
        temp_df2=pd.DataFrame()
        allRes[str(code)]={}
        originalGraph=nx.read_gpickle("gpickles/"+code+".gpickle")
        graph=originalGraph

        #Graph theoretic measures
        graph = max(nx.weakly_connected_component_subgraphs(originalGraph), key=len) # get around the problem of disconnected graphs # Refs: https://stackoverflow.com/questions/26637644/in-r-how-do-igraph-and-statnet-handle-disconnected-graphs-in-measuring-network, http://reports-archive.adm.cs.cmu.edu/anon/isr2011/CMU-ISR-11-113.pdf,
        
        if len(graph) >= 3:
            eigenCentrality=nx.eigenvector_centrality_numpy(graph) # get around the problem of failing when there are multiple eigenvalues with the same (largest) magnitude, perhaps when there are few peripheral nodes (star-like graph). See: https://stackoverflow.com/questions/43208737/using-networkx-to-calculate-eigenvector-centrality?rq=1
            hubs,authorities=nx.hits(graph, max_iter=10000, tol=1.0e-7, normalized=True) #changed parameters so that calculation convergences. For alternative approach see: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.415.843&rep=rep1&type=pdf. Now running into divide by zero error, not sure how to fix that apart from remove normalization; this would be very wrong though. Edit: increased max_iter to 10000 and this seems to have fixed the problem. NB: the original publication suggests a max_iter of 20
            degreeCentrality=nx.degree_centrality(graph)
            cfCentrality=nx.current_flow_closeness_centrality(graph.to_undirected())
            eccentCentrality=nx.eccentricity(graph.to_undirected())
            betweenCentrality=nx.betweenness_centrality(graph)
            
            for iteration in range(1, maxReps):
                allRes[str(code)][str(iteration)]={}
                pickleFile=str('pickles/'+code+'_'+str(iteration)+'_local1.pickle')
                outputList=pickle.load(open(pickleFile, 'rb')) #python2 version #outputList=pickle.load(open(pickleFile, 'rb'), encoding='latin1') #python3 version
                bruteOut1,dev,storeModel, storeModel3, equivalents, dev2 = [outputList[k] for k in range(len(outputList))]
                model1=modelHolder(storeModel3)
                
                if os.path.isfile("pickles/'+code+'_'+str(iteration)+'_scores1.pickle"):
                    pathVals=pickle.Unpickler(open('pickles/'+code+'_'+str(iteration)+'_scores1.pickle', "rb")).load()
                    ImportanceVals={}
                else:
                    print("Importance scores not found, setting all values to 0")
                    isGeneric4=True
                    pathVals=[0] * len(model1.nodeList)

                for node in range(0,len(model1.nodeList)):
                    allRes[str(code)][str(iteration)][str(model1.nodeList[node])]=[]
                    ImportanceVals=pathVals[node]
                    inDegree=originalGraph.in_degree(model1.nodeList[node])
                    if model1.nodeList[node] in graph.nodes(): #remember that we have just selected the largest component of the graph for graph theoretic analysis
                        allRes[str(code)][str(iteration)][str(model1.nodeList[node])]=[ImportanceVals, degreeCentrality[model1.nodeList[node]], eigenCentrality[model1.nodeList[node]], hubs[model1.nodeList[node]], authorities[model1.nodeList[node]], inDegree, cfCentrality[model1.nodeList[node]], eccentCentrality[model1.nodeList[node]], betweenCentrality[model1.nodeList[node]]]
                    else:
                        allRes[str(code)][str(iteration)][str(model1.nodeList[node])]=[ImportanceVals, float('NaN'), float('NaN'), float('NaN'), float('NaN'), inDegree, float('NaN'),  float('NaN'), float('NaN')]
        else:
            continue
    

    allRes_flat=flatdict.FlatDict(allRes)
    allRes_df=pd.DataFrame(allRes_flat.iteritems())

    allRes_df[["ImportanceVals", "degreeCentrality", "eigenCentrality", "hubs", "auth", "inDegree", "cfCentrality", "eccentCentrality", "betweenCentrality"]]=pd.DataFrame([item for sublist in allRes_df[[1]].values for item in sublist], index=allRes_df.index)
    allRes_df[["Pathway", "Iteration", "Node"]]=pd.DataFrame([x[0].split(":", 2) for x in allRes_df[[0]].values], index=allRes_df.index)
    allRes_df[["ImportanceVals", "degreeCentrality", "eigenCentrality", "hubs", "auth", "inDegree", "cfCentrality", "eccentCentrality", "betweenCentrality"]]=allRes_df[["ImportanceVals", "degreeCentrality", "eigenCentrality", "hubs", "auth", "inDegree", "cfCentrality","eccentCentrality", "betweenCentrality"]].apply(pd.to_numeric, axis=1)
    
    # Aggregate results by iteration
    allRes_df=allRes_df.groupby(['Node', 'Pathway']) 
    allRes_df=allRes_df["ImportanceVals", "degreeCentrality", "eigenCentrality", "hubs", "auth", "cfCentrality", "eccentCentrality", "betweenCentrality"].agg(np.mean)
        # Overall Pearson correlation between importance metrics
    sns.set_context(context='paper', font_scale=1.1)
    sns.set_style("ticks")
    #fig, ax = plt.subplots(figsize=[5.2,4])
    temp_correl=allRes_df.loc[:, ["ImportanceVals", "degreeCentrality", "eigenCentrality", "hubs", "auth", "cfCentrality", "eccentCentrality", "betweenCentrality"]].corr(method='pearson')
    mask=np.triu(temp_correl, k=1)
    figTemp=sns.heatmap(temp_correl, xticklabels=["BONITA Score", "Degree Centrality", "Eigenvector Centrality", "Hub Score", "Authority", "Current Flow Centrality", "Eccentricity Centrality", "Betweenness Centrality"], yticklabels=["BONITA Score", "Degree Centrality", "Eigenvector Centrality", "Hub Score", "Authority", "Current Flow Centrality", "Eccentricity Centrality", "Betweenness Centrality"], mask=mask, square=True, vmax=1, vmin=-1, center=0, cmap='RdBu_r', linewidths=.5, cbar_kws={"shrink": .5, 'label': 'Pearson Correlation'}, annot=True)#, annot_kws={'fontsize': 'large'})

    plt.xticks(rotation=90)
    figTemp.figure.tight_layout()
    figTemp=figTemp.get_figure()
    figTemp.savefig("Overall_Pearson_correlation.svg")
    plt.close()

    # Overall Spearman correlation between importance metrics
    sns.set_context(context='paper', font_scale=1.1)
    sns.set_style("ticks")
    #fig, ax = plt.subplots(figsize=[5.2,4])
    temp_correl=allRes_df.loc[:, ["ImportanceVals", "degreeCentrality", "eigenCentrality", "hubs", "auth", "cfCentrality", "eccentCentrality", "betweenCentrality"]].corr(method='spearman')
    mask=np.triu(temp_correl, k=1)
    figTemp=sns.heatmap(temp_correl, xticklabels=["BONITA Score", "Degree Centrality", "Eigenvector Centrality", "Hub Score", "Authority", "Current Flow Centrality", "Eccentricity Centrality", "Betweenness Centrality"], yticklabels=["BONITA Score", "Degree Centrality", "Eigenvector Centrality", "Hub Score", "Authority", "Current Flow Centrality", "Eccentricity Centrality", "Betweenness Centrality"], mask=mask, square=True, vmax=1, vmin=-1, center=0, cmap='RdBu_r', linewidths=.5, cbar_kws={"shrink": .5, 'label': 'Spearman Correlation'}, annot=True)#, annot_kws={'fontsize': 'large'})

    plt.xticks(rotation=90)
    figTemp.figure.tight_layout()
    figTemp=figTemp.get_figure()
    figTemp.savefig("Overall_Spearman_correlation.svg")
    plt.close()

def uploadKEGGcodes_hsa(codelist, graph, hsaDict, KEGGdict):
    #queries the KEGG for the pathways with the given codes then uploads to graph. Need to provide the KEGGdict so that we can name the nodes with gene names rather than KO numbers
    #Upload KEGG codes modified for human pathways
    #Updated for KEGG paths experiment, which uses the entire set of KEGG pathways from MSigDb. Some of these had no kgml and the request returned a 404 error
    #This code catches the error and continues on through the provided set of  gmt pathways
    for code in codelist:
        try:
            url=urllib2.urlopen('http://rest.kegg.jp/get/'+code+'/kgml')
            text=url.readlines()
            nc.readKEGGhsa(text, graph, hsaDict, KEGGdict)
        except urllib2.HTTPError, e:
            print(e.code)
            print(e.msg)
            continue

def read_gmt(filename):
    # Utility - reads in a gmt file
    gmt_dict={}
    inputfile = open(filename, 'r')
    lines = inputfile.readlines()
    for line in lines:
        newline=line.split('\t')
        gmt_dict[newline[0]]=Set(newline[2:])
    return gmt_dict

def retrieveGraph(name,aliasDict,dict1,dict2):
    # Utility for supplementaryFigure4
    # download and prepare graph for finding the rules
    print(name)
    # use KEGG API to figure out what the pathway code is
    namelist=name.split('_')
    namelist.pop(0)
    requester='http://rest.kegg.jp/find/pathway/'+namelist.pop(0)
    for item in namelist:
        requester=requester+'+'+item
    r=requests.get(requester)
    lines=r.text
    # parse file that comes from KEGG
    if len(lines.split('\n')[0].split(':'))>1:
        code=lines.split('\n')[0].split(':')[1][3:8] # KEGG number of overlapped pathway
        graph=nx.DiGraph()
        # download and integrate human and generic versions of pathway
        print(code)
        coder=str('ko'+code) 
        nc.uploadKEGGcodes([coder], graph, dict2)
        coder=str('hsa'+code)
        uploadKEGGcodes_hsa([coder], graph, dict1, dict2)
        # check to see if there is a connected component, simplify graph and print if so
        if len(list(nx.connected_component_subgraphs(graph.to_undirected())))>0:
            return(graph.to_directed())
        else:
            return(graph)
    else:
        print('not found:')
        print(requester)
        print(lines)
        return(nx.DiGraph()) #return an empty digraph

def supplement_S3(pathwayGraphs):
    #Utility for supplementaryFigure3
    pathwayGraphs=pickle.Unpickler(open('pathwayGraphs.pickle', "rb" )).load()
    #Same as python3 version. Takes a dictionary of name:graph
    #Get number of test networks:
    print("Number of test networks: ", len(pathwayGraphs.keys()))
    fh=open("testSimParams.csv", "w+")
    fh.write("".join(["Pathway_ID,", "Length_of_shortest_longest_path,", "Number_of_nodes_in_pathway,","Number_of_source_nodes,","Mean_out_degree_of_source_nodes", "\n"]))
    validNets=0 #counter for networks that actually have a kgml
    #For all pathway graphs
    for name, path in pathwayGraphs.items():
        print(name, path)
        print(validNets)
        if nx.number_of_nodes(path)==0:
            validNets=validNets
            fh.write("".join([str(name), ",", str("Not defined"), ",", str(0), ",", str("Not defined"),",", str("Not defined"), "\n"]))
            continue
        else:
            #Find number of nodes
            numberNodes=nx.number_of_nodes(path)
            #Find source nodes
            sourceNodes=[]
            inDegree=path.in_degree()
            for node, indeg in inDegree.items():
                if(indeg==0):
                    sourceNodes.append(node)
            #Find out-degree of source nodes
            sourceOutDeg=[]
            for x in sourceNodes:
                sourceOutDeg.append(path.out_degree(x))
            #Find mean out-degree of source nodes
            if(len(sourceOutDeg)>=1):
                sourceOutDeg_mean=np.mean(np.array(sourceOutDeg))
            else:
                sourceOutDeg_mean="Not defined"
            print(sourceOutDeg_mean)
            #Find longest shortest path
            tempMax=-1
            temp=dict(nx.all_pairs_shortest_path_length(path, cutoff=None))
            for x,y in temp.items():
                if(max(y.values())>= tempMax):
                    tempMax=max(y.values())
            fh.write("".join([str(name), ",", str(tempMax), ",", str(numberNodes), ",", str(len(sourceNodes)),",", str(sourceOutDeg_mean), "\n"]))
            validNets=validNets+1
    print("Number of networks that have KGMLs: ", validNets)
    fh.close()

def pipeline_fig_s3(filename):
    #Utility for supplementaryFigure3
    #python2 version, uses a GMT file as input
    aliasDict, dict1, dict2={}, {}, {} # set up dicts for reading KEGG files
    nc.parseKEGGdicthsa('inputData/hsa00001.keg',aliasDict,dict1)#read in kegg gene symbol dictionaries
    nc.parseKEGGdict('inputData/ko00001.keg',aliasDict,dict2)
    keggDict=read_gmt(filename) # read in GMT file
    namelist=keggDict.keys() # retain all pathways
    pathwayGraphs={} #dictionary of digraphs
    for name in namelist:
        pathwayGraphs[name]=retrieveGraph(name,aliasDict,dict1,dict2) # find and store gpickles for graphs found
    pickle.dump(pathwayGraphs, open( 'pathwayGraphs.pickle', "wb" ) ) # save data in correct format
    pathwayGraphs=pickle.Unpickler(open('pathwayGraphs.pickle', "rb" )).load()
    supplement_S3(pathwayGraphs)

def filterInts(test):
    #Utility for supplementaryFigure3
    test=str(test)
    return(test.replace('.','',1).isdigit())

def makeHist(resFile):
    #Utility for supplementaryFigure3
    resFile=pd.read_csv(resFile)
    sns.set_palette(sns.color_palette("Greys_r", 6))
    temp1=filter(filterInts, list(resFile.loc[:,'Length_of_shortest_longest_path'].values))
    temp1=[int(t) for t in temp1]
    temp1=np.array(temp1)
    sns.set_style("ticks")
    fig, ax = plt.subplots(figsize=[2.6,2])
    plt.rc('font', size=8)          # controls default text sizes
    plt.rc('axes', titlesize=8)     # fontsize of the axes title
    plt.rc('axes', labelsize=8)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=8)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=8)    # fontsize of the tick labels
    plt.rc('legend', fontsize=8)    # legend fontsize
    plt.rc('figure', titlesize=8)  # fontsize of the figure title
    b_width = 1  # chose an arbitrary value here
    my_bins = np.arange(min(temp1), max(temp1) + b_width, b_width)
    ax=sns.distplot(temp1, kde=False, hist_kws={"alpha": 1, "color": "grey"}, bins=my_bins)
    plt.legend(bbox_to_anchor=(1.1, .8), bbox_transform=plt.gcf().transFigure)
    plt.tight_layout()
    sns.set(font_scale=1) 
    plt.rc('font', size=8) # controls default text sizes
    plt.rc('axes', titlesize=8) # fontsize of the axes title
    plt.rc('axes', labelsize=8) # fontsize of the x and y labels
    plt.rc('xtick', labelsize=8) # fontsize of the tick labels
    plt.rc('ytick', labelsize=8) # fontsize of the tick labels
    plt.rc('legend', fontsize=8)  # legend fontsize
    plt.rc('figure', titlesize=8) # fontsize of the figure title
    ax=ax.get_figure()
    finishPlot("Length of longest shortest path", "Frequency", "keggCycles.png") #finishPlot(xlabeler, ylabeler, plotname)
    plt.close()

def printStats(resFile):

    #Utility for supplementaryFigure3
    #Pathway_ID,Length_of_shortest_longest_path,Number_of_nodes_in_pathway,Number_of_source_nodes,Mean_out_degree_of_source_nodes
    statDict={}
    resFile=pd.read_csv(resFile)
    statDict["testedNets"]=len(set(resFile.loc[:,'Pathway_ID'].values))
    statDict["validNets"]=len(filter(filterInts, list(resFile.loc[:,'Length_of_shortest_longest_path'].values)))
    statDict["meanLength"]=np.mean(np.array([int(t) for t in filter(filterInts, list(resFile.loc[:,'Length_of_shortest_longest_path'].values))]))
    statDict["maxNetSize"]=max([int(t) for t in filter(filterInts, list(resFile.loc[:,'Number_of_nodes_in_pathway'].values))])
    statDict["minNetSize"]=min([int(t) for t in filter(filterInts, list(resFile.loc[:,'Number_of_nodes_in_pathway'].values))])
    statDict["meanOutDegreeOfSource"]=np.mean(np.array([float(t) for t in filter(filterInts, list(resFile.loc[:,'Mean_out_degree_of_source_nodes'].values))]))
    for key, value in statDict.items():
        print(key, value)

def supplementaryFigure3():
    # Wrapper for S3
    # determination of number of simulation steps in BONITA-NP for KEGG networks
    pipeline_fig_s3("c2.cp.kegg.v6.2.symbols.gmt")
    #troubleshoot()
    makeHist('testSimParams.csv')
    printStats('testSimParams.csv')

def makeRNplots(codes, iterations=25):

    #wrapper to generate S1, S3, S4

    #allRes=scoreERS(codes, iterations)
    #allRes_df=makeRNdataframe(allRes, ifPrint=True)
    #supplementary1(allRes_df) #make supplementary figure 1
    #allRes_df=allRes_df["ruleLengths", "ImportanceVals", "scoreFunction6"].agg(np.mean)
    #Fig2PanelD(allRes_df) # Make plot corresponding to Fig2 Panel D of main text - The log average total size of ERS for each node are plotted against the log total ancestor overlap which is the sum of the pairwise shared ancestor number between any two upstream nodes
    supplementaryFigure3()
    #supplementary4() #RSV experiments, importance scores not calculated for generic4
    #codes=getCodes()
    #maxReps=25 #maximum number of iterations run, usually 5. 25 iterations were run for experiments on simulated networks.
    #makeRNplots(codes, iterations=maxReps)
#################

if __name__== "__main__":
    supplementaryFigure3()