{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of rules inferred by Bonita\n",
    "#### Author: MG Palshikar\n",
    "#### Date: 13 October, 2020\n",
    "#### Goal: Unpack the *local_1.pickle* files created by Bonita into a human-readable format\n",
    "\n",
    "***A set of output files ending with *local_1.pickle* are generated during the rule inference part of the Bonita pipeline. They contain model specifications and equivalent rules for each trial (ie, for each iteration of each network).\n",
    "This analysis code collects data for each node, from each network/iteration and collates that data into a single dataframe. The dataframe includes information on the equivalent rules and network structure and is saved as a CSV file that can be opened in Excel or a text editor.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages and functions from the Bonita package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import cPickle as pickle\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import simulation\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"import other pieces of Bonita software - utils.py, networkConstructor.py, analysis_accuracy.py\"\"\"\n",
    "from utils import *\n",
    "import networkConstructor as nc\n",
    "from analysis_accuracy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPathwayName (hsaURL):\n",
    "    \"\"\"Use KEGG API to get the readable name using the KEGG code (eg. hsa00010 corresponds to the glycolysis pathway)\"\"\"\n",
    "    fileReg = re.compile('NAME\\s+(\\w+.*)')\n",
    "    pathwayFile = requests.get('http://rest.kegg.jp/get/'+hsaURL, stream = True)\n",
    "    for line in pathwayFile.iter_lines():\n",
    "        result = fileReg.match(line)\n",
    "        if result:\n",
    "            return result.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEnds2(model, node, indiv):\n",
    "    \"\"\" find the end of a node in the bitstring \"\"\"\n",
    "    node=model.nodeList.index(node)\n",
    "    if node==len(model.nodeList)-1:\n",
    "        end1=len(indiv)\n",
    "    else:\n",
    "        end1=model.individualParse[node+1]\n",
    "    start1=model.individualParse[node]\n",
    "    return start1, end1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeName(x, nodeList):\n",
    "    return(nodeList[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def writeNode2(currentNode,nodeIndividual, model):\n",
    "\t\"\"\"write out evaluation instructions in BooleanNet format.\"\"\"\n",
    "\t# This follows the exact same code as updateNode (for switch=0), but writes a string instead of actually updating the values of the nodes\n",
    "\tandNodes=model.andNodeList[currentNode] # find the list of shadow and nodes we must compute before computing value of current nodes\n",
    "\tandNodeInvertList=model.andNodeInvertList[currentNode] #find list of lists of whether input nodes need to be inverted (corresponds to inputOrder)\n",
    "\twritenode=''+model.nodeList[currentNode]+'*=' # set up the initial string to use to write node\n",
    "\tif model.andLenList[currentNode]==0 or sum(nodeIndividual)==0:\n",
    "\t\treturn writenode + ' ' + model.nodeList[currentNode] #if no inputs, maintain value\n",
    "\telif len(andNodes)==1: \n",
    "\t\t#if only one input, then can either affect or not affect the node. so either keep the value or update to the single input's value\n",
    "\t\tvalue=''\t\n",
    "\t\t#if only one input, then set to that number\n",
    "\t\tif andNodeInvertList[0][0]==0:\n",
    "\t\t\tvalue= value + model.nodeList[andNodes[0][0]]\n",
    "\t\telse:\n",
    "\t\t\tvalue= value+ 'not ' + model.nodeList[andNodes[0][0]]\n",
    "\t\treturn writenode + value \n",
    "\telse:\n",
    "\t\t#update nodes with more than one input\n",
    "\n",
    "\t\t# first deal with case of simple logic without need of linear regression\n",
    "\t\torset=[]\n",
    "\t\t# go through list of possible shadow and nodes to see which ones actually contribute\n",
    "\t\tfor andindex in range(len(nodeIndividual)):\n",
    "\t\t\tnewval='('\n",
    "\t\t\tif nodeIndividual[andindex]==1:\n",
    "\t\t\t\t# if a shadow and contributes, compute its value using its upstream nodes\n",
    "\t\t\t\tif andNodeInvertList[andindex][0]:\n",
    "\t\t\t\t\tnewval=newval+'not '\n",
    "\t\t\t\tnewval=newval+model.nodeList[andNodes[andindex][0]]\n",
    "\t\t\t\tfor addnode in range(1,len(andNodes[andindex])):\n",
    "\t\t\t\t\tnewval= newval + ' and '\n",
    "\t\t\t\t\tif andNodeInvertList[andindex][addnode]:\n",
    "\t\t\t\t\t\tnewval=newval+' not '\n",
    "\t\t\t\t\tnewval=newval+model.nodeList[andNodes[andindex][addnode]]\n",
    "\t\t\t\torset.append(newval +')')\n",
    "\t\t\t#combine the shadow and nodes with or operations\n",
    "\t\twritenode=writenode + orset.pop()\n",
    "\t\tfor val in orset:\n",
    "\t\t\twritenode = writenode + ' or ' + val\n",
    "\t\t#print(writenode)\n",
    "\t\treturn writenode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the *local1.pickle* files in the specified directory (change directory variable below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "directory = \"\" #os.getcwd() - use current working directory\n",
    "outputfiles = []\n",
    "\n",
    "for root, dirs, files in os.walk(directory): \n",
    "    for f in files:\n",
    "        if f.endswith(\"_local1.pickle\"): #if re.compile('.*hsa05418\\_.+\\local1.pickle').match(f):\n",
    "            outputfiles.append(os.path.join(root,f))\n",
    "\n",
    "print(outputfiles[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open local1.pickle files and process the information into a single dataframe\n",
    "**One row in the dataframe contains information for one node. The dataframe has the following columns:**\n",
    " - Network name - readable, descriptive KEGG network name\n",
    " - Method name - subfolder of the main directory in which the pickle was found\n",
    " - andNodeList - indices of parent nodes\n",
    " - andNodeInvertList - a bitstring encoding the activation and inhibition edges. True implies that the edge from the corresponding parent node in the andNodeList is an inhibitory edge\n",
    " - ruleLengths - length (ie, size) of the ERS for the node\n",
    " - equivs - bitstring representation of the equivalent rule set\n",
    " - plainRules - plain text representation of the rules in the ERS\n",
    " - randomERSIndividual - random individual from the ERS\n",
    " - minLocalSearchError - lowest error for the rules tried for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileReg = re.compile('.*hsa(\\d+)\\_.+\\.pickle')\n",
    "seriesIndices = [\"networkName\", \"methodName\", \"nodeList\", \"andNodeList\", \"andNodeInvertList\", \"ruleLengths\", \"equivs\", \"plainRules\", \"randomERSIndividual\", \"minLocalSearchError\"]\n",
    "df = pd.DataFrame(columns = seriesIndices)\n",
    "i = 1\n",
    "\n",
    "fileReg2 = re.compile(re.escape(directory) + r\"(\\w+.*)\")\n",
    "                      \n",
    "for f in outputfiles[0:9]:\n",
    "    getMethod=fileReg2.match(f)\n",
    "    if getMethod:\n",
    "        methodName=getMethod.group(1)\n",
    "    else:\n",
    "        methodName=\"N.A.\"\n",
    "    result = fileReg.match(f)\n",
    "    networkName = getPathwayName('hsa'+result.group(1))\n",
    "    outputList = pickle.load(open(f, mode = \"rb\"))\n",
    "    bruteOut1,dev,storeModel, storeModel3, equivalents, dev2 = [outputList[k] for k in range(len(outputList))]\n",
    "    randomERSIndividual = bruteOut1 #random individual from the ERS\n",
    "    minLocalSearchError = dev2 #lowest error for the rules tried for each node\n",
    "    #equivalents = ERS for the network\n",
    "    #storeModel = model from the GA\n",
    "    minGAErrors = dev # minimum errors returned by the GA\n",
    "    model1=modelHolder(storeModel3) #model from the local search\n",
    "    for node in range(0,len(model1.nodeList)):\n",
    "        plainRules=[]\n",
    "        start1,end1=findEnds2(model1, model1.nodeList[node], equivalents[node])\n",
    "        ers=equivalents[node] # find the bitstring for just this node\n",
    "        inEdges=findInEdges(model1, model1.nodeList.index(model1.nodeList[node]))\n",
    "        for rule in ers:\n",
    "            plainRules.append(writeNode2(model1.nodeList.index(model1.nodeList[node]), rule, model1))\n",
    "        ruleLengths=len(ers)\n",
    "        ersAllNodes=plainRules\n",
    "        s = pd.Series([networkName, methodName, model1.nodeList[node], model1.andNodeList[node], model1.andNodeInvertList[node], ruleLengths, str(ers), plainRules, randomERSIndividual, minLocalSearchError[node]], index = seriesIndices)\n",
    "        df.loc[i] = s\n",
    "        i = i + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save this dataframe to a CSV file with specified name (change 'CSVfile' variable below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>networkName</th>\n",
       "      <th>methodName</th>\n",
       "      <th>nodeList</th>\n",
       "      <th>andNodeList</th>\n",
       "      <th>andNodeInvertList</th>\n",
       "      <th>ruleLengths</th>\n",
       "      <th>equivs</th>\n",
       "      <th>plainRules</th>\n",
       "      <th>randomERSIndividual</th>\n",
       "      <th>minLocalSearchError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [networkName, methodName, nodeList, andNodeList, andNodeInvertList, ruleLengths, equivs, plainRules, randomERSIndividual, minLocalSearchError]\n",
       "Index: []"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSVfile = \"local1Data.csv\"\n",
    "dfCSVFile = open(CSVfile, mode = \"w\")\n",
    "df.to_csv(dfCSVFile)\n",
    "dfCSVFile.close()\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
